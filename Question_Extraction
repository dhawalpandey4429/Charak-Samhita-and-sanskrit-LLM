# üì¶ Install dependencies
!pip install transformers sentencepiece torch pandas tqdm

# üìö Import libraries
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from tqdm import tqdm
import torch
import pandas as pd

# ‚úÖ Load model
model_name = "google/mt5-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# ‚úÖ Load cleaned Sanskrit text
input_path = "/content/sanskrit_cleaned_segmented.txt"
with open(input_path, "r", encoding="utf-8") as f:
    text = f.read()

# ‚úÖ Split into chunks
chunks = [chunk.strip() for chunk in text.split("\n\n") if len(chunk.strip()) > 20]

# ‚úÖ Question generation
def generate_question(text, prefix="‡§∏‡§ô‡•ç‡§ï‡•ç‡§∑‡§ø‡§™‡•ç‡§§ ‡§™‡•ç‡§∞‡§∂‡•ç‡§®‡§É ‡§®‡§ø‡§∞‡•ç‡§Æ‡•Ä‡§Ø‡§§‡§æ‡§Æ‡•ç: ", max_length=128):
    input_text = prefix + text
    inputs = tokenizer.encode(input_text, return_tensors="pt", max_length=512, truncation=True)
    outputs = model.generate(inputs, max_length=max_length, num_beams=4, early_stopping=True)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# ‚úÖ Simple Sanskrit answer extractor
def extract_answer(context, question):
    sentences = [s.strip() for s in context.split("‡•§") if len(s.strip()) > 10]
    return sentences[0] + "‡•§" if sentences else context.strip()

# ‚úÖ Sanskrit question classifier (rule-based)
def classify_question_sanskrit(question):
    question = question.strip()

    factual = ["‡§ï‡§ø‡§Æ‡•ç", "‡§ï‡§É", "‡§ï‡•á", "‡§ï‡§∏‡•ç‡§Æ‡§ø‡§®‡•ç", "‡§ï‡•ã", "‡§ï‡§æ‡§®‡§ø", "‡§ï‡§∏‡•ç‡§Ø"]
    conceptual = ["‡§ï‡§•‡§Æ‡•ç", "‡§ï‡•Å‡§§‡§É", "‡§ï‡§∏‡•ç‡§Æ‡§æ‡§§‡•ç", "‡§ï‡§ø‡§Ç ‡§ï‡§æ‡§∞‡§£‡§Æ‡•ç", "‡§∏‡•ç‡§™‡§∑‡•ç‡§ü‡•Ä‡§ï‡•Å‡§∞‡•Å", "‡§ï‡•á‡§® ‡§™‡•ç‡§∞‡§ï‡§æ‡§∞‡•á‡§£"]
    thematic = ["‡§ï‡§æ ‡§≠‡•Ç‡§Æ‡§ø‡§ï‡§æ", "‡§ï‡§É ‡§§‡§æ‡§§‡•ç‡§™‡§∞‡•ç‡§Ø‡§Æ‡•ç", "‡§µ‡§ø‡§ö‡§æ‡§∞‡§Ø", "‡§Ü‡§≤‡•á‡§ñ‡§Ø", "‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£‡§Ç", "‡§µ‡§∞‡•ç‡§£‡§Ø"]

    for word in conceptual:
        if word in question:
            return "‡§µ‡§ø‡§ö‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï‡§Æ‡•ç"

    for word in thematic:
        if word in question:
            return "‡§µ‡§ø‡§∑‡§Ø‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡•ç"

    for word in factual:
        if word in question:
            return "‡§§‡§•‡•ç‡§Ø‡§æ‡§§‡•ç‡§Æ‡§ï‡§Æ‡•ç"

    return "‡§§‡§•‡•ç‡§Ø‡§æ‡§§‡•ç‡§Æ‡§ï‡§Æ‡•ç"  # Default fallback

# ‚úÖ Process everything
data = []

for i, chunk in enumerate(tqdm(chunks, desc="üìò Generating QA Pairs with Labels")):
    try:
        question = generate_question(chunk)
        answer = extract_answer(chunk, question)
        label = classify_question_sanskrit(question)

        data.append({
            "context": chunk,
            "question": question,
            "answer": answer,
            "label": label
        })

    except Exception as e:
        print(f"‚ö†Ô∏è Error at chunk {i+1}: {e}")

# ‚úÖ Save to CSV
output_csv_path = "/content/sanskrit_qa_classified_dataset.csv"
df = pd.DataFrame(data)
df.to_csv(output_csv_path, index=False, encoding="utf-8-sig")
print(f"\n‚úÖ Labeled QA dataset saved at: {output_csv_path}")

from google.colab import files
files.download("/content/sanskrit_qa_classified_dataset.csv")
